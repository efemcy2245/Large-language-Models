{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\david\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["transformers.models.t5.tokenization_t5_fast.T5TokenizerFast"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import transformers\n","from transformers import AutoTokenizer\n","\n","model_name = \"google-t5/t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","type(tokenizer)  #### recognize the class type"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##### how to tokenize a sentence\n","\n","sentence = \"hello, this is a sentence!\"\n","\n","tokens = tokenizer.__call__(sentence)        # is a dictionary keys: input_ids - attention_mask\n","print(tokens)\n","\n","### we can inverse the process, using the decoder \n","tokenizer.decode(tokens[\"input_ids\"])   ## is added the term </s> to indicate the end of the sequence"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random \n","\n","vocabulary = tokenizer.get_vocab()   ### to get the entire set of tokens \n","print(len(vocabulary))               ### the size is 32100 tokens\n","\n","\n","random_elem = random.sample(range(0, len(vocabulary) + 1), 10)\n","print(random_elem)\n","\n","reverse_vocab = {id: word for word, id in vocabulary.items()}  #### we map from word- integer to integer-word\n","\n","vocab_keys = list(reverse_vocab.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_token = vocabulary[\"</s>\"]\n","print(num_token)\n","print(tokens[\"input_ids\"])  ### the last token is 1 the </s>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer(\"hello!</s></s>\")  # we have 1 more 1 value, because the tokenizer add it"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer.tokenize(sentence) ### we can see how is tokenize the sentence"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(tokenizer.tokenize(\"hello    ,world\"))     # multiple spaces are compacted into a single one!\n","print(tokenizer.tokenize(\"hello    , world\"))    # for world we have 2 different mapping"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" \n","Special attributes are available in the tokenizer class to access these special tokens. Some examples are:\n","\n","pad_token is the token used for padding (as discussed later),\n","bos_token and eos_token tokens are used to indicate the beginning and end of the input text, respectively,\n","mask_token is used for masking tokens during training (e.g., for the masked LM task, with BERT),\n","sep_token is used to separate sentences in the input text (e.g., next sentence prediction, with BERT),\n","cls_token is used to indicate the beginning of the input text (e.g., for classification tasks, with BERT),\n","unk_token is used to indicate unknown tokens (i.e., tokens that are not in the vocabulary).\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# if the model don't use that special token then is set to 0\n","tokenizer.eos_token, tokenizer.pad_token, tokenizer.bos_token"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# corresponding id\n","tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.bos_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentences = [\n","    \"this is the first sentence\",\n","    \"instead, this is the second sequence!\"\n","]\n","\n","\n","tokens = tokenizer.__call__(sentences)\n","print(tokens)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Now each sentence has the same length\n","tokens = tokenizer.__call__(sentences, padding=True)\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
